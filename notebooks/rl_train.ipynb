{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from baselines.common.vec_env import VecEnvWrapper, SubprocVecEnv\n",
    "from pred_learn.envs import *\n",
    "from a2c_ppo_acktr.envs import VecPyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2c_ppo_acktr import algo\n",
    "from a2c_ppo_acktr.arguments import get_args\n",
    "from a2c_ppo_acktr.envs import make_vec_envs\n",
    "from a2c_ppo_acktr.model import Policy\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "from a2c_ppo_acktr.utils import get_vec_normalize, update_linear_schedule\n",
    "from a2c_ppo_acktr.visualize import visdom_plot\n",
    "from collections import deque\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from skimage.transform import resize\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 500\n",
    "TOTAL_STEPS = 10000\n",
    "ENVS = [\n",
    "    \"CarRacing-v0\",\n",
    "    \"Snake-ple-v0\",\n",
    "    \"TetrisA-v2\",\n",
    "    \"PuckWorld-ple-v0\",\n",
    "    \"WaterWorld-ple-v0\",\n",
    "    \"PixelCopter-ple-v0\",\n",
    "    \"CubeCrash-v0\",\n",
    "    \"Catcher-ple-v0\",\n",
    "    \"Pong-ple-v0\",\n",
    "]\n",
    "ENV_ID = \"PixelCopter-ple-v0\"\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# RECORD_DIR = \"recorded/{}/\".format(ENV_ID)\n",
    "# FILE_NO = 1\n",
    "\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(RECORD_DIR)\n",
    "# except FileExistsError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ira/code/envs/flexi/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ConcatNoise<PLEEnv<PixelCopter-ple-v0>>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_env(ENV_ID, SEED)\n",
    "env._env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.cuda = True\n",
    "args.vis = False\n",
    "args.num_processes = 4\n",
    "args.seed = SEED\n",
    "\n",
    "args.env_name = ENV_ID\n",
    "\n",
    "args.gamma = 0.99\n",
    "args.log_dir = \"tests\"\n",
    "args.log_interval = 1\n",
    "args.eval_interval = 10\n",
    "eval_log_dir = args.log_dir + \"_eval\"\n",
    "args.add_timestep = False\n",
    "args.add_padding = False\n",
    "args.recurrent_policy = False\n",
    "args.algo = \"a2c\"\n",
    "args.value_loss_coef = 0.5\n",
    "args.entropy_coef = 0.01\n",
    "args.lr = 7e-4\n",
    "args.eps = 1e-5\n",
    "args.alpha = 0.99\n",
    "args.max_grad_norm = 0.5\n",
    "args.num_steps = 15\n",
    "args.use_linear_lr_decay = False\n",
    "args.use_gae = False\n",
    "args.tau = 0.95\n",
    "args.clip_param = 0.2\n",
    "args.ppo_epoch = 4\n",
    "args.num_mini_batch = 32\n",
    "args.use_linear_clip_decay = False\n",
    "args.save_interval = 1000\n",
    "args.save_dir = \"save_dir\"\n",
    "\n",
    "num_updates = 100000\n",
    "\n",
    "try:\n",
    "    os.makedirs(args.log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(args.log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "        \n",
    "try:\n",
    "    os.makedirs(eval_log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(eval_log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)\n",
    "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
    "\n",
    "if args.vis:\n",
    "    from visdom import Visdom\n",
    "    viz = Visdom(port=args.port)\n",
    "    win = None\n",
    "\n",
    "envs = make_vec_envs(args.env_name, args.seed, args.num_processes,\n",
    "                     args.gamma, args.log_dir, args.add_timestep, device, extra_detail=False)\n",
    "\n",
    "# envs = make_rl_envs(args.env_name, args.num_processes, args.seed, device)\n",
    "    \n",
    "actor_critic = Policy(envs.observation_space.shape, envs.action_space,\n",
    "    base_kwargs={'recurrent': args.recurrent_policy})\n",
    "actor_critic.to(device)\n",
    "\n",
    "if args.algo == 'a2c':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, args.value_loss_coef,\n",
    "                           args.entropy_coef, lr=args.lr,\n",
    "                           eps=args.eps, alpha=args.alpha,\n",
    "                           max_grad_norm=args.max_grad_norm)\n",
    "elif args.algo == 'ppo':\n",
    "    agent = algo.PPO(actor_critic, args.clip_param, args.ppo_epoch, args.num_mini_batch,\n",
    "                     args.value_loss_coef, args.entropy_coef, lr=args.lr,\n",
    "                           eps=args.eps,\n",
    "                           max_grad_norm=args.max_grad_norm)\n",
    "elif args.algo == 'acktr':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, args.value_loss_coef,\n",
    "                           args.entropy_coef, acktr=True)\n",
    "\n",
    "rollouts = RolloutStorage(args.num_steps, args.num_processes,\n",
    "                    envs.observation_space.shape, envs.action_space,\n",
    "                    actor_critic.recurrent_hidden_state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_model = torch.load(\"../../pytorch-a2c-ppo-acktr/trained_models/ppo/CarRacing-v0.pt\")\n",
    "# actor_critic, _ = some_model\n",
    "# actor_critic = actor_critic.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 0, num timesteps 60, FPS 447 \n",
      " Last 7 training episodes: mean/median reward -4.9/-5.0, min/max reward -5.0/-4.0\n",
      "\n",
      " Evaluation using 11 episodes: mean reward -5.00000\n",
      "\n",
      "Updates 1, num timesteps 120, FPS 39 \n",
      " Last 10 training episodes: mean/median reward -5.0/-5.0, min/max reward -5.0/-5.0\n",
      "\n",
      "Updates 2, num timesteps 180, FPS 57 \n",
      " Last 10 training episodes: mean/median reward -5.0/-5.0, min/max reward -5.0/-5.0\n",
      "\n",
      "Updates 3, num timesteps 240, FPS 74 \n",
      " Last 10 training episodes: mean/median reward -5.0/-5.0, min/max reward -5.0/-5.0\n",
      "\n",
      "Updates 4, num timesteps 300, FPS 90 \n",
      " Last 10 training episodes: mean/median reward -4.7/-5.0, min/max reward -5.0/-3.0\n",
      "\n",
      "Updates 5, num timesteps 360, FPS 105 \n",
      " Last 10 training episodes: mean/median reward -5.0/-5.0, min/max reward -5.0/-5.0\n",
      "\n",
      "Updates 6, num timesteps 420, FPS 120 \n",
      " Last 10 training episodes: mean/median reward -5.0/-5.0, min/max reward -5.0/-5.0\n",
      "\n",
      "Updates 7, num timesteps 480, FPS 134 \n",
      " Last 10 training episodes: mean/median reward -5.0/-5.0, min/max reward -5.0/-5.0\n",
      "\n",
      "Updates 8, num timesteps 540, FPS 147 \n",
      " Last 10 training episodes: mean/median reward -4.1/-5.0, min/max reward -5.0/-1.0\n",
      "\n",
      "Updates 9, num timesteps 600, FPS 160 \n",
      " Last 10 training episodes: mean/median reward -4.0/-4.5, min/max reward -5.0/-1.0\n",
      "\n",
      "Updates 10, num timesteps 660, FPS 172 \n",
      " Last 10 training episodes: mean/median reward -4.3/-5.0, min/max reward -5.0/-2.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward -3.00000\n",
      "\n",
      "Updates 11, num timesteps 720, FPS 96 \n",
      " Last 10 training episodes: mean/median reward -4.2/-4.5, min/max reward -5.0/-2.0\n",
      "\n",
      "Updates 12, num timesteps 780, FPS 103 \n",
      " Last 10 training episodes: mean/median reward -3.9/-4.5, min/max reward -5.0/-2.0\n",
      "\n",
      "Updates 13, num timesteps 840, FPS 110 \n",
      " Last 10 training episodes: mean/median reward -3.6/-3.5, min/max reward -5.0/-1.0\n",
      "\n",
      "Updates 14, num timesteps 900, FPS 117 \n",
      " Last 10 training episodes: mean/median reward -3.3/-3.0, min/max reward -5.0/-1.0\n",
      "\n",
      "Updates 15, num timesteps 960, FPS 124 \n",
      " Last 10 training episodes: mean/median reward -3.9/-4.0, min/max reward -5.0/-2.0\n",
      "\n",
      "Updates 16, num timesteps 1020, FPS 130 \n",
      " Last 10 training episodes: mean/median reward -3.9/-4.0, min/max reward -5.0/-2.0\n",
      "\n",
      "Updates 17, num timesteps 1080, FPS 137 \n",
      " Last 10 training episodes: mean/median reward -3.7/-4.0, min/max reward -5.0/-2.0\n",
      "\n",
      "Updates 18, num timesteps 1140, FPS 143 \n",
      " Last 10 training episodes: mean/median reward -3.2/-4.0, min/max reward -5.0/0.0\n",
      "\n",
      "Updates 19, num timesteps 1200, FPS 149 \n",
      " Last 10 training episodes: mean/median reward -3.1/-3.5, min/max reward -5.0/0.0\n",
      "\n",
      "Updates 20, num timesteps 1260, FPS 155 \n",
      " Last 10 training episodes: mean/median reward -3.0/-3.0, min/max reward -5.0/0.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward -3.00000\n",
      "\n",
      "Updates 21, num timesteps 1320, FPS 116 \n",
      " Last 10 training episodes: mean/median reward -2.4/-3.0, min/max reward -5.0/0.0\n",
      "\n",
      "Updates 22, num timesteps 1380, FPS 120 \n",
      " Last 10 training episodes: mean/median reward -3.1/-3.5, min/max reward -5.0/0.0\n",
      "\n",
      "Updates 23, num timesteps 1440, FPS 124 \n",
      " Last 10 training episodes: mean/median reward -3.3/-3.5, min/max reward -5.0/0.0\n",
      "\n",
      "Updates 24, num timesteps 1500, FPS 128 \n",
      " Last 10 training episodes: mean/median reward -3.5/-4.0, min/max reward -5.0/0.0\n",
      "\n",
      "Updates 25, num timesteps 1560, FPS 133 \n",
      " Last 10 training episodes: mean/median reward -3.0/-3.5, min/max reward -5.0/1.0\n",
      "\n",
      "Updates 26, num timesteps 1620, FPS 137 \n",
      " Last 10 training episodes: mean/median reward -2.8/-3.0, min/max reward -5.0/1.0\n",
      "\n",
      "Updates 27, num timesteps 1680, FPS 141 \n",
      " Last 10 training episodes: mean/median reward -1.8/-2.5, min/max reward -4.0/1.0\n",
      "\n",
      "Updates 28, num timesteps 1740, FPS 145 \n",
      " Last 10 training episodes: mean/median reward -1.8/-2.5, min/max reward -4.0/1.0\n",
      "\n",
      "Updates 29, num timesteps 1800, FPS 149 \n",
      " Last 10 training episodes: mean/median reward -1.9/-2.5, min/max reward -4.0/1.0\n",
      "\n",
      "Updates 30, num timesteps 1860, FPS 153 \n",
      " Last 10 training episodes: mean/median reward -2.1/-2.5, min/max reward -5.0/1.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward -3.00000\n",
      "\n",
      "Updates 31, num timesteps 1920, FPS 126 \n",
      " Last 10 training episodes: mean/median reward -2.6/-3.0, min/max reward -5.0/-1.0\n",
      "\n",
      "Updates 32, num timesteps 1980, FPS 129 \n",
      " Last 10 training episodes: mean/median reward -2.9/-3.0, min/max reward -5.0/-1.0\n",
      "\n",
      "Updates 33, num timesteps 2040, FPS 132 \n",
      " Last 10 training episodes: mean/median reward -2.9/-3.0, min/max reward -5.0/-1.0\n",
      "\n",
      "Updates 34, num timesteps 2100, FPS 135 \n",
      " Last 10 training episodes: mean/median reward -2.4/-3.0, min/max reward -5.0/5.0\n",
      "\n",
      "Updates 35, num timesteps 2160, FPS 139 \n",
      " Last 10 training episodes: mean/median reward -2.2/-3.0, min/max reward -4.0/5.0\n",
      "\n",
      "Updates 36, num timesteps 2220, FPS 142 \n",
      " Last 10 training episodes: mean/median reward -1.8/-3.0, min/max reward -4.0/5.0\n",
      "\n",
      "Updates 37, num timesteps 2280, FPS 145 \n",
      " Last 10 training episodes: mean/median reward -1.7/-2.5, min/max reward -4.0/5.0\n",
      "\n",
      "Updates 38, num timesteps 2340, FPS 148 \n",
      " Last 10 training episodes: mean/median reward -2.2/-2.5, min/max reward -4.0/0.0\n",
      "\n",
      "Updates 39, num timesteps 2400, FPS 151 \n",
      " Last 10 training episodes: mean/median reward -2.1/-2.0, min/max reward -5.0/1.0\n",
      "\n",
      "Updates 40, num timesteps 2460, FPS 154 \n",
      " Last 10 training episodes: mean/median reward -2.3/-2.5, min/max reward -5.0/1.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d9b9e5dfaa1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m         eval_envs = make_vec_envs(\n\u001b[1;32m    112\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             args.gamma, eval_log_dir, args.add_timestep, device, True)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mvec_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vec_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/projects/pytorch-a2c-ppo-acktr/a2c_ppo_acktr/envs.py\u001b[0m in \u001b[0;36mmake_vec_envs\u001b[0;34m(env_name, seed, num_processes, gamma, log_dir, add_timestep, device, allow_early_resets, num_frame_stack, padding_type)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubprocVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/cool-code/baselines/baselines/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fns, spaces, context)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get_spaces_spec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mVecEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "envs = make_vec_envs(args.env_name, args.seed, args.num_processes,\n",
    "                     args.gamma, args.log_dir, args.add_timestep, device, True,\n",
    "                     padding_type=args.add_padding)\n",
    "\n",
    "# envs = make_rl_envs(args.env_name, args.num_processes, args.seed, device)\n",
    "\n",
    "\n",
    "obs = envs.reset()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "episode_rewards = deque(maxlen=10)\n",
    "\n",
    "training_stats = {\n",
    "    'updates': [],\n",
    "    'num_timesteps': [],\n",
    "    'mean_reward': [],\n",
    "    'median_reward': [],\n",
    "    'min_reward': [],\n",
    "    'max_reward': []\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "for j in range(num_updates):\n",
    "\n",
    "    if args.use_linear_lr_decay:\n",
    "        # decrease learning rate linearly\n",
    "        if args.algo == \"acktr\":\n",
    "            # use optimizer's learning rate since it's hard-coded in kfac.py\n",
    "            update_linear_schedule(agent.optimizer, j, num_updates, agent.optimizer.lr)\n",
    "        else:\n",
    "            update_linear_schedule(agent.optimizer, j, num_updates, args.lr)\n",
    "\n",
    "    if args.algo == 'ppo' and args.use_linear_clip_decay:\n",
    "        agent.clip_param = args.clip_param  * (1 - j / float(num_updates))\n",
    "\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step],\n",
    "                    rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, infos = envs.step(action)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                   for done_ in done])\n",
    "        rollouts.insert(obs, recurrent_hidden_states, action, action_log_prob, value, reward, masks)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = actor_critic.get_value(rollouts.obs[-1],\n",
    "                                            rollouts.recurrent_hidden_states[-1],\n",
    "                                            rollouts.masks[-1]).detach()\n",
    "\n",
    "    rollouts.compute_returns(next_value, args.use_gae, args.gamma, args.tau)\n",
    "\n",
    "    value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "\n",
    "    rollouts.after_update()\n",
    "\n",
    "    # save for every interval-th episode or for the last epoch\n",
    "    if (j % args.save_interval == 0 or j == num_updates - 1) and args.save_dir != \"\":\n",
    "        save_path = os.path.join(args.save_dir, args.algo)\n",
    "        try:\n",
    "            os.makedirs(save_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        # A really ugly way to save a model to CPU\n",
    "        save_model = actor_critic\n",
    "        if args.cuda:\n",
    "            save_model = copy.deepcopy(actor_critic).cpu()\n",
    "\n",
    "        save_model = [save_model,\n",
    "                      getattr(get_vec_normalize(envs), 'ob_rms', None)]\n",
    "\n",
    "        torch.save(save_model, os.path.join(save_path, args.env_name + \".pt\"))\n",
    "\n",
    "    total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "\n",
    "#     if j % args.log_interval == 0 and len(episode_rewards) > 1:\n",
    "    if len(episode_rewards) > 1:\n",
    "        end = time.time()\n",
    "        print(\"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\".\n",
    "            format(j, total_num_steps,\n",
    "                   int(total_num_steps / (end - start)),\n",
    "                   len(episode_rewards),\n",
    "                   np.mean(episode_rewards),\n",
    "                   np.median(episode_rewards),\n",
    "                   np.min(episode_rewards),\n",
    "                   np.max(episode_rewards), dist_entropy,\n",
    "                   value_loss, action_loss))\n",
    "\n",
    "        training_stats['updates'].append(j)\n",
    "        training_stats['num_timesteps'].append(total_num_steps)\n",
    "        training_stats['mean_reward'].append(np.mean(episode_rewards))\n",
    "        training_stats['median_reward'].append(np.median(episode_rewards))\n",
    "        training_stats['min_reward'].append(np.min(episode_rewards))\n",
    "        training_stats['max_reward'].append(np.max(episode_rewards))\n",
    "\n",
    "    if (args.eval_interval is not None\n",
    "            and len(episode_rewards) > 1\n",
    "            and j % args.eval_interval == 0):\n",
    "        eval_envs = make_vec_envs(\n",
    "            args.env_name, args.seed + args.num_processes, args.num_processes,\n",
    "            args.gamma, eval_log_dir, args.add_timestep, device, True)\n",
    "\n",
    "        vec_norm = get_vec_normalize(eval_envs)\n",
    "        if vec_norm is not None:\n",
    "            vec_norm.eval()\n",
    "            vec_norm.ob_rms = get_vec_normalize(envs).ob_rms\n",
    "\n",
    "        eval_episode_rewards = []\n",
    "\n",
    "        obs = eval_envs.reset()\n",
    "        eval_recurrent_hidden_states = torch.zeros(args.num_processes,\n",
    "                        actor_critic.recurrent_hidden_state_size, device=device)\n",
    "        eval_masks = torch.zeros(args.num_processes, 1, device=device)\n",
    "\n",
    "        while len(eval_episode_rewards) < 10:\n",
    "            with torch.no_grad():\n",
    "                _, action, _, eval_recurrent_hidden_states = actor_critic.act(\n",
    "                    obs, eval_recurrent_hidden_states, eval_masks, deterministic=True)\n",
    "\n",
    "            # Obser reward and next obs\n",
    "            obs, reward, done, infos = eval_envs.step(action)\n",
    "\n",
    "            eval_masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                            for done_ in done]).to(device)\n",
    "            for info in infos:\n",
    "                if info is not None and 'episode' in info.keys():\n",
    "                    eval_episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        eval_envs.close()\n",
    "\n",
    "        print(\" Evaluation using {} episodes: mean reward {:.5f}\\n\".\n",
    "            format(len(eval_episode_rewards),\n",
    "                   np.mean(eval_episode_rewards)))\n",
    "\n",
    "    if args.vis and j % args.vis_interval == 0:\n",
    "        try:\n",
    "            # Sometimes monitor doesn't properly flush the outputs\n",
    "            win = visdom_plot(viz, win, args.log_dir, args.env_name,\n",
    "                              args.algo, args.num_env_steps)\n",
    "        except IOError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = make_rl_envs(args.env_name, args.num_processes, args.seed,  device)\n",
    "\n",
    "obs = envs.reset()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "episode_rewards = deque(maxlen=10)\n",
    "\n",
    "training_stats = {\n",
    "    'updates': [],\n",
    "    'num_timesteps': [],\n",
    "    'mean_reward': [],\n",
    "    'median_reward': [],\n",
    "    'min_reward': [],\n",
    "    'max_reward': []\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "for j in range(50):\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step],\n",
    "                    rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, infos = envs.step(action)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                   for done_ in done])\n",
    "        rollouts.insert(obs, recurrent_hidden_states, action, action_log_prob, value, reward, masks)\n",
    "        \n",
    "#         obs = obs.values().clone()\n",
    "        obs = torch.FloatTensor(obs.cpu().data).cuda()\n",
    "        obs.requires_grad = True\n",
    "        next_value = actor_critic.get_value(obs,\n",
    "                                    rollouts.recurrent_hidden_states[-1],\n",
    "                                    rollouts.masks[-1])\n",
    "        next_value[0, ...].mean().backward()        \n",
    "        \n",
    "        im_display = np.concatenate(np.split(obs[0, ...].detach().cpu().numpy().transpose([1, 2, 0]).astype('uint8'), 4, axis=2), axis=1)\n",
    "        \n",
    "        print(\"reward\", reward[0])\n",
    "        print(\"action\", action[0])\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(im_display, cmap=\"gray\")        \n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(obs.grad[0,...].sum(dim=0).abs().cpu().numpy(), cmap=\"gray\")        \n",
    "        \n",
    "        plt.pause(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_display = np.concatenate(np.split(obs[0, ...].detach().cpu().numpy().transpose([1, 2, 0]).astype('uint8'), 4, axis=2), axis=1)\n",
    "plt.imshow(im_display)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
