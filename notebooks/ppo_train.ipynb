{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "from baselines.common.vec_env import VecEnvWrapper, SubprocVecEnv\n",
    "from pred_learn.envs import *\n",
    "from a2c_ppo_acktr.envs import VecPyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2c_ppo_acktr import algo\n",
    "from a2c_ppo_acktr.arguments import get_args\n",
    "from a2c_ppo_acktr.envs import make_vec_envs\n",
    "from a2c_ppo_acktr.model import Policy\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "from a2c_ppo_acktr.utils import get_vec_normalize, update_linear_schedule\n",
    "from a2c_ppo_acktr.visualize import visdom_plot\n",
    "from collections import deque\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from skimage.transform import resize\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 500\n",
    "TOTAL_STEPS = 10000\n",
    "ENVS = [\n",
    "    \"CarRacing-v0\",\n",
    "    \"Snake-ple-v0\",\n",
    "    \"TetrisA-v2\",\n",
    "    \"PuckWorld-ple-v0\",\n",
    "    \"WaterWorld-ple-v0\",\n",
    "    \"PixelCopter-ple-v0\",\n",
    "    \"CubeCrash-v0\",\n",
    "    \"Catcher-ple-v0\",\n",
    "    \"Pong-ple-v0\",\n",
    "]\n",
    "ENV_ID = 'MountainCar-v0'\n",
    "SEED = 0\n",
    "\n",
    "# RECORD_DIR = \"recorded/{}/\".format(ENV_ID)\n",
    "# FILE_NO = 1\n",
    "\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(RECORD_DIR)\n",
    "# except FileExistsError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(ENV_ID, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-inf, inf)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._env.reward_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.cuda = True\n",
    "args.vis = False\n",
    "args.num_processes = 4\n",
    "args.seed = SEED\n",
    "\n",
    "args.env_name = ENV_ID\n",
    "\n",
    "args.gamma = 0.99\n",
    "args.log_dir = \"tests\"\n",
    "args.log_interval = 1\n",
    "args.eval_interval = 10\n",
    "eval_log_dir = args.log_dir + \"_eval\"\n",
    "args.add_timestep = False\n",
    "args.add_padding = False\n",
    "args.recurrent_policy = False\n",
    "args.algo = \"ppo\"\n",
    "args.value_loss_coef = 0.5\n",
    "args.entropy_coef = 0.01\n",
    "args.lr = 7e-4\n",
    "args.eps = 1e-5\n",
    "args.alpha = 0.99\n",
    "args.max_grad_norm = 0.5\n",
    "args.num_steps = 15\n",
    "args.use_linear_lr_decay = False\n",
    "args.use_gae = False\n",
    "args.tau = 0.95\n",
    "args.clip_param = 0.2\n",
    "args.ppo_epoch = 4\n",
    "args.num_mini_batch = 32\n",
    "args.use_linear_clip_decay = False\n",
    "args.save_interval = 1000\n",
    "args.save_dir = \"save_dir\"\n",
    "\n",
    "num_updates = 100000\n",
    "\n",
    "try:\n",
    "    os.makedirs(args.log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(args.log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "        \n",
    "try:\n",
    "    os.makedirs(eval_log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(eval_log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)\n",
    "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
    "\n",
    "if args.vis:\n",
    "    from visdom import Visdom\n",
    "    viz = Visdom(port=args.port)\n",
    "    win = None\n",
    "\n",
    "envs = make_vec_envs(args.env_name, args.seed, args.num_processes,\n",
    "                     args.gamma, args.log_dir, args.add_timestep, device, True,\n",
    "                     padding_type=args.add_padding)\n",
    "\n",
    "# envs = make_rl_envs(args.env_name, args.num_processes, args.seed, device)\n",
    "    \n",
    "actor_critic = Policy(envs.observation_space.shape, envs.action_space,\n",
    "    base_kwargs={'recurrent': args.recurrent_policy})\n",
    "actor_critic.to(device)\n",
    "\n",
    "if args.algo == 'a2c':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, args.value_loss_coef,\n",
    "                           args.entropy_coef, lr=args.lr,\n",
    "                           eps=args.eps, alpha=args.alpha,\n",
    "                           max_grad_norm=args.max_grad_norm)\n",
    "elif args.algo == 'ppo':\n",
    "    agent = algo.PPO(actor_critic, args.clip_param, args.ppo_epoch, args.num_mini_batch,\n",
    "                     args.value_loss_coef, args.entropy_coef, lr=args.lr,\n",
    "                           eps=args.eps,\n",
    "                           max_grad_norm=args.max_grad_norm)\n",
    "elif args.algo == 'acktr':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, args.value_loss_coef,\n",
    "                           args.entropy_coef, acktr=True)\n",
    "\n",
    "rollouts = RolloutStorage(args.num_steps, args.num_processes,\n",
    "                    envs.observation_space.shape, envs.action_space,\n",
    "                    actor_critic.recurrent_hidden_state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 13, num timesteps 840, FPS 32 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 14, num timesteps 900, FPS 32 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 15, num timesteps 960, FPS 32 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 16, num timesteps 1020, FPS 32 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 17, num timesteps 1080, FPS 32 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 18, num timesteps 1140, FPS 32 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 19, num timesteps 1200, FPS 32 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 20, num timesteps 1260, FPS 32 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      " Evaluation using 12 episodes: mean reward -200.00000\n",
      "\n",
      "Updates 21, num timesteps 1320, FPS 19 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 22, num timesteps 1380, FPS 20 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 23, num timesteps 1440, FPS 20 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 24, num timesteps 1500, FPS 20 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 25, num timesteps 1560, FPS 21 \n",
      " Last 4 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 26, num timesteps 1620, FPS 21 \n",
      " Last 8 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 27, num timesteps 1680, FPS 21 \n",
      " Last 8 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n",
      "Updates 28, num timesteps 1740, FPS 21 \n",
      " Last 8 training episodes: mean/median reward -200.0/-200.0, min/max reward -200.0/-200.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d9b9e5dfaa1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/projects/pytorch-a2c-ppo-acktr/a2c_ppo_acktr/algo/ppo.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 (value_loss * self.value_loss_coef + action_loss -\n\u001b[0;32m---> 78\u001b[0;31m                  dist_entropy * self.entropy_coef).backward()\n\u001b[0m\u001b[1;32m     79\u001b[0m                 nn.utils.clip_grad_norm_(self.actor_critic.parameters(),\n\u001b[1;32m     80\u001b[0m                                          self.max_grad_norm)\n",
      "\u001b[0;32m~/code/envs/flexi/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/envs/flexi/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "envs = make_vec_envs(args.env_name, args.seed, args.num_processes,\n",
    "                     args.gamma, args.log_dir, args.add_timestep, device, True,\n",
    "                     padding_type=args.add_padding)\n",
    "\n",
    "# envs = make_rl_envs(args.env_name, args.num_processes, args.seed, device)\n",
    "\n",
    "\n",
    "obs = envs.reset()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "episode_rewards = deque(maxlen=10)\n",
    "\n",
    "training_stats = {\n",
    "    'updates': [],\n",
    "    'num_timesteps': [],\n",
    "    'mean_reward': [],\n",
    "    'median_reward': [],\n",
    "    'min_reward': [],\n",
    "    'max_reward': []\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "for j in range(num_updates):\n",
    "\n",
    "    if args.use_linear_lr_decay:\n",
    "        # decrease learning rate linearly\n",
    "        if args.algo == \"acktr\":\n",
    "            # use optimizer's learning rate since it's hard-coded in kfac.py\n",
    "            update_linear_schedule(agent.optimizer, j, num_updates, agent.optimizer.lr)\n",
    "        else:\n",
    "            update_linear_schedule(agent.optimizer, j, num_updates, args.lr)\n",
    "\n",
    "    if args.algo == 'ppo' and args.use_linear_clip_decay:\n",
    "        agent.clip_param = args.clip_param  * (1 - j / float(num_updates))\n",
    "\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step],\n",
    "                    rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, infos = envs.step(action)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                   for done_ in done])\n",
    "        rollouts.insert(obs, recurrent_hidden_states, action, action_log_prob, value, reward, masks)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = actor_critic.get_value(rollouts.obs[-1],\n",
    "                                            rollouts.recurrent_hidden_states[-1],\n",
    "                                            rollouts.masks[-1]).detach()\n",
    "\n",
    "    rollouts.compute_returns(next_value, args.use_gae, args.gamma, args.tau)\n",
    "\n",
    "    value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "\n",
    "    rollouts.after_update()\n",
    "\n",
    "    # save for every interval-th episode or for the last epoch\n",
    "    if (j % args.save_interval == 0 or j == num_updates - 1) and args.save_dir != \"\":\n",
    "        save_path = os.path.join(args.save_dir, args.algo)\n",
    "        try:\n",
    "            os.makedirs(save_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        # A really ugly way to save a model to CPU\n",
    "        save_model = actor_critic\n",
    "        if args.cuda:\n",
    "            save_model = copy.deepcopy(actor_critic).cpu()\n",
    "\n",
    "        save_model = [save_model,\n",
    "                      getattr(get_vec_normalize(envs), 'ob_rms', None)]\n",
    "\n",
    "        torch.save(save_model, os.path.join(save_path, args.env_name + \".pt\"))\n",
    "\n",
    "    total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "\n",
    "#     if j % args.log_interval == 0 and len(episode_rewards) > 1:\n",
    "    if len(episode_rewards) > 1:\n",
    "        end = time.time()\n",
    "        print(\"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\".\n",
    "            format(j, total_num_steps,\n",
    "                   int(total_num_steps / (end - start)),\n",
    "                   len(episode_rewards),\n",
    "                   np.mean(episode_rewards),\n",
    "                   np.median(episode_rewards),\n",
    "                   np.min(episode_rewards),\n",
    "                   np.max(episode_rewards), dist_entropy,\n",
    "                   value_loss, action_loss))\n",
    "\n",
    "        training_stats['updates'].append(j)\n",
    "        training_stats['num_timesteps'].append(total_num_steps)\n",
    "        training_stats['mean_reward'].append(np.mean(episode_rewards))\n",
    "        training_stats['median_reward'].append(np.median(episode_rewards))\n",
    "        training_stats['min_reward'].append(np.min(episode_rewards))\n",
    "        training_stats['max_reward'].append(np.max(episode_rewards))\n",
    "\n",
    "    if (args.eval_interval is not None\n",
    "            and len(episode_rewards) > 1\n",
    "            and j % args.eval_interval == 0):\n",
    "        eval_envs = make_vec_envs(\n",
    "            args.env_name, args.seed + args.num_processes, args.num_processes,\n",
    "            args.gamma, eval_log_dir, args.add_timestep, device, True)\n",
    "\n",
    "        vec_norm = get_vec_normalize(eval_envs)\n",
    "        if vec_norm is not None:\n",
    "            vec_norm.eval()\n",
    "            vec_norm.ob_rms = get_vec_normalize(envs).ob_rms\n",
    "\n",
    "        eval_episode_rewards = []\n",
    "\n",
    "        obs = eval_envs.reset()\n",
    "        eval_recurrent_hidden_states = torch.zeros(args.num_processes,\n",
    "                        actor_critic.recurrent_hidden_state_size, device=device)\n",
    "        eval_masks = torch.zeros(args.num_processes, 1, device=device)\n",
    "\n",
    "        while len(eval_episode_rewards) < 10:\n",
    "            with torch.no_grad():\n",
    "                _, action, _, eval_recurrent_hidden_states = actor_critic.act(\n",
    "                    obs, eval_recurrent_hidden_states, eval_masks, deterministic=True)\n",
    "\n",
    "            # Obser reward and next obs\n",
    "            obs, reward, done, infos = eval_envs.step(action)\n",
    "\n",
    "            eval_masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                            for done_ in done]).to(device)\n",
    "            for info in infos:\n",
    "                if info is not None and 'episode' in info.keys():\n",
    "                    eval_episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        eval_envs.close()\n",
    "\n",
    "        print(\" Evaluation using {} episodes: mean reward {:.5f}\\n\".\n",
    "            format(len(eval_episode_rewards),\n",
    "                   np.mean(eval_episode_rewards)))\n",
    "\n",
    "    if args.vis and j % args.vis_interval == 0:\n",
    "        try:\n",
    "            # Sometimes monitor doesn't properly flush the outputs\n",
    "            win = visdom_plot(viz, win, args.log_dir, args.env_name,\n",
    "                              args.algo, args.num_env_steps)\n",
    "        except IOError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d3ed5b91be45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/envs/flexi/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mpause\u001b[0;34m(interval)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/envs/flexi/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mstart_event_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2258\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_looping\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2260\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2261\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "envs = make_rl_envs(args.env_name, args.num_processes, args.seed,  device)\n",
    "\n",
    "obs = envs.reset()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "episode_rewards = deque(maxlen=10)\n",
    "\n",
    "training_stats = {\n",
    "    'updates': [],\n",
    "    'num_timesteps': [],\n",
    "    'mean_reward': [],\n",
    "    'median_reward': [],\n",
    "    'min_reward': [],\n",
    "    'max_reward': []\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "for j in range(50):\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step],\n",
    "                    rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, infos = envs.step(action)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                   for done_ in done])\n",
    "        rollouts.insert(obs, recurrent_hidden_states, action, action_log_prob, value, reward, masks)\n",
    "        \n",
    "#         obs = obs.values().clone()\n",
    "        obs = torch.FloatTensor(obs.cpu().data).cuda()\n",
    "        obs.requires_grad = True\n",
    "        next_value = actor_critic.get_value(obs,\n",
    "                                    rollouts.recurrent_hidden_states[-1],\n",
    "                                    rollouts.masks[-1])\n",
    "        next_value[0, ...].mean().backward()        \n",
    "        \n",
    "        print(\"reward\", reward[0])\n",
    "        print(\"action\", action[0])\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(obs.detach().cpu().numpy()[0, -1, ...], cmap=\"gray\")        \n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(obs.grad[0,...].sum(dim=0).abs().cpu().numpy(), cmap=\"gray\")        \n",
    "        \n",
    "        plt.pause(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
