{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "from baselines.common.vec_env import VecEnvWrapper, SubprocVecEnv\n",
    "from pred_learn.envs import *\n",
    "from a2c_ppo_acktr.envs import VecPyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2c_ppo_acktr import algo\n",
    "from a2c_ppo_acktr.arguments import get_args\n",
    "from a2c_ppo_acktr.envs import make_vec_envs\n",
    "from a2c_ppo_acktr.model import Policy\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "from a2c_ppo_acktr.utils import get_vec_normalize, update_linear_schedule\n",
    "from a2c_ppo_acktr.visualize import visdom_plot\n",
    "from collections import deque\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from skimage.transform import resize\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 500\n",
    "TOTAL_STEPS = 10000\n",
    "ENVS = [\n",
    "    \"CarRacing-v0\",\n",
    "    \"Snake-ple-v0\",\n",
    "    \"TetrisA-v2\",\n",
    "    \"PuckWorld-ple-v0\",\n",
    "    \"WaterWorld-ple-v0\",\n",
    "    \"PixelCopter-ple-v0\",\n",
    "    \"CubeCrash-v0\",\n",
    "    \"Catcher-ple-v0\",\n",
    "    \"Pong-ple-v0\",\n",
    "]\n",
    "ENV_ID = \"PuckWorld-ple-v0\"\n",
    "SEED = 0\n",
    "\n",
    "# RECORD_DIR = \"recorded/{}/\".format(ENV_ID)\n",
    "# FILE_NO = 1\n",
    "\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(RECORD_DIR)\n",
    "# except FileExistsError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ira/code/envs/flexi/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = make_env(ENV_ID, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-inf, inf)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._env.reward_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.cuda = True\n",
    "args.vis = False\n",
    "args.num_processes = 4\n",
    "args.seed = SEED\n",
    "\n",
    "args.env_name = ENV_ID\n",
    "\n",
    "args.gamma = 0.999\n",
    "args.log_dir = \"tests\"\n",
    "args.log_interval = 1\n",
    "args.eval_interval = 10\n",
    "eval_log_dir = args.log_dir + \"_eval\"\n",
    "args.add_timestep = False\n",
    "args.add_padding = False\n",
    "args.recurrent_policy = False\n",
    "args.algo = \"ppo\"\n",
    "args.value_loss_coef = 0.5\n",
    "args.entropy_coef = 0.01\n",
    "args.lr = 7e-4\n",
    "args.eps = 1e-5\n",
    "args.alpha = 0.99\n",
    "args.max_grad_norm = 0.5\n",
    "args.num_steps = 15\n",
    "args.use_linear_lr_decay = False\n",
    "args.use_gae = False\n",
    "args.tau = 0.95\n",
    "args.clip_param = 0.2\n",
    "args.ppo_epoch = 4\n",
    "args.num_mini_batch = 32\n",
    "args.use_linear_clip_decay = False\n",
    "args.save_interval = 1000\n",
    "args.save_dir = \"save_dir\"\n",
    "\n",
    "num_updates = 100000\n",
    "\n",
    "try:\n",
    "    os.makedirs(args.log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(args.log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "        \n",
    "try:\n",
    "    os.makedirs(eval_log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(eval_log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)\n",
    "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
    "\n",
    "if args.vis:\n",
    "    from visdom import Visdom\n",
    "    viz = Visdom(port=args.port)\n",
    "    win = None\n",
    "\n",
    "# envs = make_vec_envs(args.env_name, args.seed, args.num_processes,\n",
    "#                      args.gamma, args.log_dir, args.add_timestep, device, False,\n",
    "#                      padding_type=args.add_padding)\n",
    "\n",
    "envs = make_rl_envs(args.env_name, args.num_processes, args.seed, device)\n",
    "    \n",
    "actor_critic = Policy(envs.observation_space.shape, envs.action_space,\n",
    "    base_kwargs={'recurrent': args.recurrent_policy})\n",
    "actor_critic.to(device)\n",
    "\n",
    "if args.algo == 'a2c':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, args.value_loss_coef,\n",
    "                           args.entropy_coef, lr=args.lr,\n",
    "                           eps=args.eps, alpha=args.alpha,\n",
    "                           max_grad_norm=args.max_grad_norm)\n",
    "elif args.algo == 'ppo':\n",
    "    agent = algo.PPO(actor_critic, args.clip_param, args.ppo_epoch, args.num_mini_batch,\n",
    "                     args.value_loss_coef, args.entropy_coef, lr=args.lr,\n",
    "                           eps=args.eps,\n",
    "                           max_grad_norm=args.max_grad_norm)\n",
    "elif args.algo == 'acktr':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, args.value_loss_coef,\n",
    "                           args.entropy_coef, acktr=True)\n",
    "\n",
    "rollouts = RolloutStorage(args.num_steps, args.num_processes,\n",
    "                    envs.observation_space.shape, envs.action_space,\n",
    "                    actor_critic.recurrent_hidden_state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envs = make_vec_envs(args.env_name, args.seed, args.num_processes,\n",
    "#                      args.gamma, args.log_dir, args.add_timestep, device, False,\n",
    "#                      padding_type=args.add_padding)\n",
    "\n",
    "envs = make_rl_envs(args.env_name, args.num_processes, args.seed, device)\n",
    "\n",
    "\n",
    "obs = envs.reset()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "episode_rewards = deque(maxlen=10)\n",
    "\n",
    "training_stats = {\n",
    "    'updates': [],\n",
    "    'num_timesteps': [],\n",
    "    'mean_reward': [],\n",
    "    'median_reward': [],\n",
    "    'min_reward': [],\n",
    "    'max_reward': []\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "for j in range(num_updates):\n",
    "\n",
    "    if args.use_linear_lr_decay:\n",
    "        # decrease learning rate linearly\n",
    "        if args.algo == \"acktr\":\n",
    "            # use optimizer's learning rate since it's hard-coded in kfac.py\n",
    "            update_linear_schedule(agent.optimizer, j, num_updates, agent.optimizer.lr)\n",
    "        else:\n",
    "            update_linear_schedule(agent.optimizer, j, num_updates, args.lr)\n",
    "\n",
    "    if args.algo == 'ppo' and args.use_linear_clip_decay:\n",
    "        agent.clip_param = args.clip_param  * (1 - j / float(num_updates))\n",
    "\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step],\n",
    "                    rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, infos = envs.step(action)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                   for done_ in done])\n",
    "        rollouts.insert(obs, recurrent_hidden_states, action, action_log_prob, value, reward, masks)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = actor_critic.get_value(rollouts.obs[-1],\n",
    "                                            rollouts.recurrent_hidden_states[-1],\n",
    "                                            rollouts.masks[-1]).detach()\n",
    "\n",
    "    rollouts.compute_returns(next_value, args.use_gae, args.gamma, args.tau)\n",
    "\n",
    "    value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "\n",
    "    rollouts.after_update()\n",
    "\n",
    "    # save for every interval-th episode or for the last epoch\n",
    "    if (j % args.save_interval == 0 or j == num_updates - 1) and args.save_dir != \"\":\n",
    "        save_path = os.path.join(args.save_dir, args.algo)\n",
    "        try:\n",
    "            os.makedirs(save_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        # A really ugly way to save a model to CPU\n",
    "        save_model = actor_critic\n",
    "        if args.cuda:\n",
    "            save_model = copy.deepcopy(actor_critic).cpu()\n",
    "\n",
    "        save_model = [save_model,\n",
    "                      getattr(get_vec_normalize(envs), 'ob_rms', None)]\n",
    "\n",
    "        torch.save(save_model, os.path.join(save_path, args.env_name + \".pt\"))\n",
    "\n",
    "    total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "\n",
    "    if j % args.log_interval == 0 and len(episode_rewards) > 1:\n",
    "#     if len(episode_rewards) > 1:\n",
    "        end = time.time()\n",
    "        print(\"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\".\n",
    "            format(j, total_num_steps,\n",
    "                   int(total_num_steps / (end - start)),\n",
    "                   len(episode_rewards),\n",
    "                   np.mean(episode_rewards),\n",
    "                   np.median(episode_rewards),\n",
    "                   np.min(episode_rewards),\n",
    "                   np.max(episode_rewards), dist_entropy,\n",
    "                   value_loss, action_loss))\n",
    "\n",
    "        training_stats['updates'].append(j)\n",
    "        training_stats['num_timesteps'].append(total_num_steps)\n",
    "        training_stats['mean_reward'].append(np.mean(episode_rewards))\n",
    "        training_stats['median_reward'].append(np.median(episode_rewards))\n",
    "        training_stats['min_reward'].append(np.min(episode_rewards))\n",
    "        training_stats['max_reward'].append(np.max(episode_rewards))\n",
    "\n",
    "    if (args.eval_interval is not None\n",
    "            and len(episode_rewards) > 1\n",
    "            and j % args.eval_interval == 0):\n",
    "        eval_envs = make_vec_envs(\n",
    "            args.env_name, args.seed + args.num_processes, args.num_processes,\n",
    "            args.gamma, eval_log_dir, args.add_timestep, device, True)\n",
    "\n",
    "        vec_norm = get_vec_normalize(eval_envs)\n",
    "        if vec_norm is not None:\n",
    "            vec_norm.eval()\n",
    "            vec_norm.ob_rms = get_vec_normalize(envs).ob_rms\n",
    "\n",
    "        eval_episode_rewards = []\n",
    "\n",
    "        obs = eval_envs.reset()\n",
    "        eval_recurrent_hidden_states = torch.zeros(args.num_processes,\n",
    "                        actor_critic.recurrent_hidden_state_size, device=device)\n",
    "        eval_masks = torch.zeros(args.num_processes, 1, device=device)\n",
    "\n",
    "        while len(eval_episode_rewards) < 10:\n",
    "            with torch.no_grad():\n",
    "                _, action, _, eval_recurrent_hidden_states = actor_critic.act(\n",
    "                    obs, eval_recurrent_hidden_states, eval_masks, deterministic=True)\n",
    "\n",
    "            # Obser reward and next obs\n",
    "            obs, reward, done, infos = eval_envs.step(action)\n",
    "\n",
    "            eval_masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                            for done_ in done]).to(device)\n",
    "            for info in infos:\n",
    "                if info is not None and 'episode' in info.keys():\n",
    "                    eval_episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        eval_envs.close()\n",
    "\n",
    "        print(\" Evaluation using {} episodes: mean reward {:.5f}\\n\".\n",
    "            format(len(eval_episode_rewards),\n",
    "                   np.mean(eval_episode_rewards)))\n",
    "\n",
    "    if args.vis and j % args.vis_interval == 0:\n",
    "        try:\n",
    "            # Sometimes monitor doesn't properly flush the outputs\n",
    "            win = visdom_plot(viz, win, args.log_dir, args.env_name,\n",
    "                              args.algo, args.num_env_steps)\n",
    "        except IOError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = make_rl_envs(args.env_name, args.num_processes, args.seed,  device)\n",
    "\n",
    "obs = envs.reset()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "episode_rewards = deque(maxlen=10)\n",
    "\n",
    "training_stats = {\n",
    "    'updates': [],\n",
    "    'num_timesteps': [],\n",
    "    'mean_reward': [],\n",
    "    'median_reward': [],\n",
    "    'min_reward': [],\n",
    "    'max_reward': []\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "for j in range(50):\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step],\n",
    "                    rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, infos = envs.step(action)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                   for done_ in done])\n",
    "        rollouts.insert(obs, recurrent_hidden_states, action, action_log_prob, value, reward, masks)\n",
    "        \n",
    "#         obs = obs.values().clone()\n",
    "        obs = torch.FloatTensor(obs.cpu().data).cuda()\n",
    "        obs.requires_grad = True\n",
    "        next_value = actor_critic.get_value(obs,\n",
    "                                    rollouts.recurrent_hidden_states[-1],\n",
    "                                    rollouts.masks[-1])\n",
    "        next_value[0, ...].mean().backward()        \n",
    "        \n",
    "        print(\"reward\", reward[0])\n",
    "        print(\"action\", action[0])\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(obs.detach().cpu().numpy()[0, -1, ...], cmap=\"gray\")        \n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(obs.grad[0,...].sum(dim=0).abs().cpu().numpy(), cmap=\"gray\")        \n",
    "        \n",
    "        plt.pause(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
