{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "from baselines.common.vec_env import VecEnvWrapper, SubprocVecEnv\n",
    "from pred_learn.envs import *\n",
    "from a2c_ppo_acktr.envs import VecPyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2c_ppo_acktr import algo\n",
    "from a2c_ppo_acktr.arguments import get_args\n",
    "from a2c_ppo_acktr.envs import make_vec_envs\n",
    "from a2c_ppo_acktr.model import Policy\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "from a2c_ppo_acktr.utils import get_vec_normalize, update_linear_schedule\n",
    "from a2c_ppo_acktr.visualize import visdom_plot\n",
    "from collections import deque\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from skimage.transform import resize\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 500\n",
    "TOTAL_STEPS = 10000\n",
    "ENVS = [\n",
    "    \"CarRacing-v0\",\n",
    "    \"Snake-ple-v0\",\n",
    "    \"TetrisA-v2\",\n",
    "    \"PuckWorld-ple-v0\",\n",
    "    \"WaterWorld-ple-v0\",\n",
    "    \"PixelCopter-ple-v0\",\n",
    "    \"CubeCrash-v0\",\n",
    "    \"Catcher-ple-v0\",\n",
    "    \"Pong-ple-v0\",\n",
    "]\n",
    "ENV_ID = 'Pong-ple-v0'\n",
    "ENV_ID = \"WaterWorld-ple-v0\"\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# RECORD_DIR = \"recorded/{}/\".format(ENV_ID)\n",
    "# FILE_NO = 1\n",
    "\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(RECORD_DIR)\n",
    "# except FileExistsError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ira/code/envs/flexi/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gym_ple.ple_env.PLEEnv at 0x7f06a98e8898>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_env(ENV_ID, SEED)\n",
    "env._env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.cuda = True\n",
    "args.vis = False\n",
    "args.num_processes = 4\n",
    "args.seed = SEED\n",
    "\n",
    "args.env_name = ENV_ID\n",
    "\n",
    "args.gamma = 0.99\n",
    "args.log_dir = \"tests\"\n",
    "args.log_interval = 1\n",
    "args.eval_interval = 10\n",
    "eval_log_dir = args.log_dir + \"_eval\"\n",
    "args.add_timestep = False\n",
    "args.add_padding = False\n",
    "args.recurrent_policy = False\n",
    "args.algo = \"a2c\"\n",
    "args.value_loss_coef = 0.5\n",
    "args.entropy_coef = 0.01\n",
    "args.lr = 7e-4\n",
    "args.eps = 1e-5\n",
    "args.alpha = 0.99\n",
    "args.max_grad_norm = 0.5\n",
    "args.num_steps = 15\n",
    "args.use_linear_lr_decay = False\n",
    "args.use_gae = False\n",
    "args.tau = 0.95\n",
    "args.clip_param = 0.2\n",
    "args.ppo_epoch = 4\n",
    "args.num_mini_batch = 32\n",
    "args.use_linear_clip_decay = False\n",
    "args.save_interval = 1000\n",
    "args.save_dir = \"save_dir\"\n",
    "\n",
    "num_updates = 100000\n",
    "\n",
    "try:\n",
    "    os.makedirs(args.log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(args.log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "        \n",
    "try:\n",
    "    os.makedirs(eval_log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(eval_log_dir, '*.monitor.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)\n",
    "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
    "\n",
    "if args.vis:\n",
    "    from visdom import Visdom\n",
    "    viz = Visdom(port=args.port)\n",
    "    win = None\n",
    "\n",
    "envs = make_vec_envs(args.env_name, args.seed, args.num_processes,\n",
    "                     args.gamma, args.log_dir, args.add_timestep, device, True,\n",
    "                     padding_type=args.add_padding)\n",
    "\n",
    "# envs = make_rl_envs(args.env_name, args.num_processes, args.seed, device)\n",
    "    \n",
    "actor_critic = Policy(envs.observation_space.shape, envs.action_space,\n",
    "    base_kwargs={'recurrent': args.recurrent_policy})\n",
    "actor_critic.to(device)\n",
    "\n",
    "if args.algo == 'a2c':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, args.value_loss_coef,\n",
    "                           args.entropy_coef, lr=args.lr,\n",
    "                           eps=args.eps, alpha=args.alpha,\n",
    "                           max_grad_norm=args.max_grad_norm)\n",
    "elif args.algo == 'ppo':\n",
    "    agent = algo.PPO(actor_critic, args.clip_param, args.ppo_epoch, args.num_mini_batch,\n",
    "                     args.value_loss_coef, args.entropy_coef, lr=args.lr,\n",
    "                           eps=args.eps,\n",
    "                           max_grad_norm=args.max_grad_norm)\n",
    "elif args.algo == 'acktr':\n",
    "    agent = algo.A2C_ACKTR(actor_critic, args.value_loss_coef,\n",
    "                           args.entropy_coef, acktr=True)\n",
    "\n",
    "rollouts = RolloutStorage(args.num_steps, args.num_processes,\n",
    "                    envs.observation_space.shape, envs.action_space,\n",
    "                    actor_critic.recurrent_hidden_state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_model = torch.load(\"../../pytorch-a2c-ppo-acktr/trained_models/ppo/CarRacing-v0.pt\")\n",
    "# actor_critic, _ = some_model\n",
    "# actor_critic = actor_critic.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 66, num timesteps 4020, FPS 404 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 67, num timesteps 4080, FPS 404 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 68, num timesteps 4140, FPS 404 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 69, num timesteps 4200, FPS 402 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 70, num timesteps 4260, FPS 401 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      " Evaluation using 11 episodes: mean reward 0.00000\n",
      "\n",
      "Updates 71, num timesteps 4320, FPS 110 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 72, num timesteps 4380, FPS 111 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 73, num timesteps 4440, FPS 113 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 74, num timesteps 4500, FPS 114 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 75, num timesteps 4560, FPS 115 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 76, num timesteps 4620, FPS 116 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 77, num timesteps 4680, FPS 117 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 78, num timesteps 4740, FPS 118 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 79, num timesteps 4800, FPS 119 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 80, num timesteps 4860, FPS 120 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 0.00000\n",
      "\n",
      "Updates 81, num timesteps 4920, FPS 71 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 82, num timesteps 4980, FPS 72 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 83, num timesteps 5040, FPS 73 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 84, num timesteps 5100, FPS 73 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 85, num timesteps 5160, FPS 74 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 86, num timesteps 5220, FPS 75 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 87, num timesteps 5280, FPS 75 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 88, num timesteps 5340, FPS 76 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 89, num timesteps 5400, FPS 77 \n",
      " Last 4 training episodes: mean/median reward 4.2/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 90, num timesteps 5460, FPS 78 \n",
      " Last 5 training episodes: mean/median reward 3.4/3.0, min/max reward 0.0/8.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 0.00000\n",
      "\n",
      "Updates 91, num timesteps 5520, FPS 60 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 92, num timesteps 5580, FPS 60 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 93, num timesteps 5640, FPS 61 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 94, num timesteps 5700, FPS 62 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 95, num timesteps 5760, FPS 62 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 96, num timesteps 5820, FPS 63 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 97, num timesteps 5880, FPS 63 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 98, num timesteps 5940, FPS 64 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 99, num timesteps 6000, FPS 65 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 100, num timesteps 6060, FPS 65 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 1.50000\n",
      "\n",
      "Updates 101, num timesteps 6120, FPS 57 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 102, num timesteps 6180, FPS 57 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 103, num timesteps 6240, FPS 58 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 104, num timesteps 6300, FPS 58 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 105, num timesteps 6360, FPS 59 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 106, num timesteps 6420, FPS 59 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 107, num timesteps 6480, FPS 60 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 108, num timesteps 6540, FPS 60 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 109, num timesteps 6600, FPS 61 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 110, num timesteps 6660, FPS 61 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 0.90000\n",
      "\n",
      "Updates 111, num timesteps 6720, FPS 49 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 112, num timesteps 6780, FPS 49 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 113, num timesteps 6840, FPS 50 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 114, num timesteps 6900, FPS 50 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 115, num timesteps 6960, FPS 51 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 116, num timesteps 7020, FPS 51 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 117, num timesteps 7080, FPS 51 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 118, num timesteps 7140, FPS 52 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 119, num timesteps 7200, FPS 52 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 120, num timesteps 7260, FPS 53 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 1.50000\n",
      "\n",
      "Updates 121, num timesteps 7320, FPS 45 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 122, num timesteps 7380, FPS 45 \n",
      " Last 6 training episodes: mean/median reward 4.0/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 123, num timesteps 7440, FPS 45 \n",
      " Last 8 training episodes: mean/median reward 3.8/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 124, num timesteps 7500, FPS 46 \n",
      " Last 8 training episodes: mean/median reward 3.8/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 125, num timesteps 7560, FPS 46 \n",
      " Last 8 training episodes: mean/median reward 3.8/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 126, num timesteps 7620, FPS 46 \n",
      " Last 8 training episodes: mean/median reward 3.8/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 127, num timesteps 7680, FPS 47 \n",
      " Last 8 training episodes: mean/median reward 3.8/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 128, num timesteps 7740, FPS 47 \n",
      " Last 8 training episodes: mean/median reward 3.8/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 129, num timesteps 7800, FPS 47 \n",
      " Last 8 training episodes: mean/median reward 3.8/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 130, num timesteps 7860, FPS 48 \n",
      " Last 8 training episodes: mean/median reward 3.8/4.5, min/max reward 0.0/8.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 1.50000\n",
      "\n",
      "Updates 131, num timesteps 7920, FPS 40 \n",
      " Last 8 training episodes: mean/median reward 3.8/4.5, min/max reward 0.0/8.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 132, num timesteps 7980, FPS 40 \n",
      " Last 8 training episodes: mean/median reward 3.8/4.5, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 133, num timesteps 8040, FPS 40 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 134, num timesteps 8100, FPS 41 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 135, num timesteps 8160, FPS 41 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 136, num timesteps 8220, FPS 41 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 137, num timesteps 8280, FPS 41 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 138, num timesteps 8340, FPS 42 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 139, num timesteps 8400, FPS 42 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 140, num timesteps 8460, FPS 42 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 1.50000\n",
      "\n",
      "Updates 141, num timesteps 8520, FPS 36 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 142, num timesteps 8580, FPS 36 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 143, num timesteps 8640, FPS 36 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 144, num timesteps 8700, FPS 37 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 145, num timesteps 8760, FPS 37 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 146, num timesteps 8820, FPS 37 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 147, num timesteps 8880, FPS 37 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 148, num timesteps 8940, FPS 38 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 149, num timesteps 9000, FPS 38 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 150, num timesteps 9060, FPS 38 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 1.50000\n",
      "\n",
      "Updates 151, num timesteps 9120, FPS 34 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 152, num timesteps 9180, FPS 34 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 153, num timesteps 9240, FPS 35 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 154, num timesteps 9300, FPS 35 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 155, num timesteps 9360, FPS 35 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 156, num timesteps 9420, FPS 35 \n",
      " Last 10 training episodes: mean/median reward 3.8/4.0, min/max reward 0.0/8.0\n",
      "\n",
      "Updates 157, num timesteps 9480, FPS 35 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 158, num timesteps 9540, FPS 36 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 159, num timesteps 9600, FPS 36 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 160, num timesteps 9660, FPS 36 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 1.50000\n",
      "\n",
      "Updates 161, num timesteps 9720, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 162, num timesteps 9780, FPS 33 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 163, num timesteps 9840, FPS 33 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 164, num timesteps 9900, FPS 33 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 165, num timesteps 9960, FPS 33 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 166, num timesteps 10020, FPS 33 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 167, num timesteps 10080, FPS 34 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 168, num timesteps 10140, FPS 34 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 169, num timesteps 10200, FPS 34 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 170, num timesteps 10260, FPS 34 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 0.80000\n",
      "\n",
      "Updates 171, num timesteps 10320, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 172, num timesteps 10380, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 173, num timesteps 10440, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 174, num timesteps 10500, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 175, num timesteps 10560, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 176, num timesteps 10620, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 177, num timesteps 10680, FPS 33 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 178, num timesteps 10740, FPS 33 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 179, num timesteps 10800, FPS 33 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 180, num timesteps 10860, FPS 33 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 1.50000\n",
      "\n",
      "Updates 181, num timesteps 10920, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 182, num timesteps 10980, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 183, num timesteps 11040, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 184, num timesteps 11100, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 185, num timesteps 11160, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 186, num timesteps 11220, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 187, num timesteps 11280, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 188, num timesteps 11340, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 189, num timesteps 11400, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 3.5/4.0, min/max reward 0.0/7.0\n",
      "\n",
      "Updates 190, num timesteps 11460, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 3.3/4.0, min/max reward -2.0/7.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 1.50000\n",
      "\n",
      "Updates 191, num timesteps 11520, FPS 30 \n",
      " Last 10 training episodes: mean/median reward 3.3/4.0, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 192, num timesteps 11580, FPS 30 \n",
      " Last 10 training episodes: mean/median reward 3.3/4.0, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 193, num timesteps 11640, FPS 30 \n",
      " Last 10 training episodes: mean/median reward 3.3/4.0, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 194, num timesteps 11700, FPS 30 \n",
      " Last 10 training episodes: mean/median reward 3.3/4.0, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 195, num timesteps 11760, FPS 30 \n",
      " Last 10 training episodes: mean/median reward 3.3/4.0, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 196, num timesteps 11820, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 3.3/4.0, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 197, num timesteps 11880, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 3.3/4.0, min/max reward -2.0/7.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 198, num timesteps 11940, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 3.3/4.0, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 199, num timesteps 12000, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 200, num timesteps 12060, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 1.50000\n",
      "\n",
      "Updates 201, num timesteps 12120, FPS 30 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 202, num timesteps 12180, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 203, num timesteps 12240, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 204, num timesteps 12300, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 205, num timesteps 12360, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 206, num timesteps 12420, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 207, num timesteps 12480, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 208, num timesteps 12540, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 209, num timesteps 12600, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 210, num timesteps 12660, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 1.50000\n",
      "\n",
      "Updates 211, num timesteps 12720, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 212, num timesteps 12780, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 213, num timesteps 12840, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 214, num timesteps 12900, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 215, num timesteps 12960, FPS 31 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 216, num timesteps 13020, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 217, num timesteps 13080, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 2.3/1.5, min/max reward -2.0/7.0\n",
      "\n",
      "Updates 218, num timesteps 13140, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 2.2/1.5, min/max reward -2.0/6.0\n",
      "\n",
      "Updates 219, num timesteps 13200, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 2.2/1.5, min/max reward -2.0/6.0\n",
      "\n",
      "Updates 220, num timesteps 13260, FPS 32 \n",
      " Last 10 training episodes: mean/median reward 2.2/1.5, min/max reward -2.0/6.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d9b9e5dfaa1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# Obser reward and next obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_envs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             eval_masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
      "\u001b[0;32m~/code/cool-code/baselines/baselines/common/vec_env/vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/projects/pytorch-a2c-ppo-acktr/a2c_ppo_acktr/envs.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstacked_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_dim0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstacked_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_dim0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/projects/pytorch-a2c-ppo-acktr/a2c_ppo_acktr/envs.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/cool-code/baselines/baselines/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/cool-code/baselines/baselines/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "envs = make_vec_envs(args.env_name, args.seed, args.num_processes,\n",
    "                     args.gamma, args.log_dir, args.add_timestep, device, True,\n",
    "                     padding_type=args.add_padding)\n",
    "\n",
    "# envs = make_rl_envs(args.env_name, args.num_processes, args.seed, device)\n",
    "\n",
    "\n",
    "obs = envs.reset()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "episode_rewards = deque(maxlen=10)\n",
    "\n",
    "training_stats = {\n",
    "    'updates': [],\n",
    "    'num_timesteps': [],\n",
    "    'mean_reward': [],\n",
    "    'median_reward': [],\n",
    "    'min_reward': [],\n",
    "    'max_reward': []\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "for j in range(num_updates):\n",
    "\n",
    "    if args.use_linear_lr_decay:\n",
    "        # decrease learning rate linearly\n",
    "        if args.algo == \"acktr\":\n",
    "            # use optimizer's learning rate since it's hard-coded in kfac.py\n",
    "            update_linear_schedule(agent.optimizer, j, num_updates, agent.optimizer.lr)\n",
    "        else:\n",
    "            update_linear_schedule(agent.optimizer, j, num_updates, args.lr)\n",
    "\n",
    "    if args.algo == 'ppo' and args.use_linear_clip_decay:\n",
    "        agent.clip_param = args.clip_param  * (1 - j / float(num_updates))\n",
    "\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step],\n",
    "                    rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, infos = envs.step(action)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                   for done_ in done])\n",
    "        rollouts.insert(obs, recurrent_hidden_states, action, action_log_prob, value, reward, masks)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = actor_critic.get_value(rollouts.obs[-1],\n",
    "                                            rollouts.recurrent_hidden_states[-1],\n",
    "                                            rollouts.masks[-1]).detach()\n",
    "\n",
    "    rollouts.compute_returns(next_value, args.use_gae, args.gamma, args.tau)\n",
    "\n",
    "    value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "\n",
    "    rollouts.after_update()\n",
    "\n",
    "    # save for every interval-th episode or for the last epoch\n",
    "    if (j % args.save_interval == 0 or j == num_updates - 1) and args.save_dir != \"\":\n",
    "        save_path = os.path.join(args.save_dir, args.algo)\n",
    "        try:\n",
    "            os.makedirs(save_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        # A really ugly way to save a model to CPU\n",
    "        save_model = actor_critic\n",
    "        if args.cuda:\n",
    "            save_model = copy.deepcopy(actor_critic).cpu()\n",
    "\n",
    "        save_model = [save_model,\n",
    "                      getattr(get_vec_normalize(envs), 'ob_rms', None)]\n",
    "\n",
    "        torch.save(save_model, os.path.join(save_path, args.env_name + \".pt\"))\n",
    "\n",
    "    total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "\n",
    "#     if j % args.log_interval == 0 and len(episode_rewards) > 1:\n",
    "    if len(episode_rewards) > 1:\n",
    "        end = time.time()\n",
    "        print(\"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\".\n",
    "            format(j, total_num_steps,\n",
    "                   int(total_num_steps / (end - start)),\n",
    "                   len(episode_rewards),\n",
    "                   np.mean(episode_rewards),\n",
    "                   np.median(episode_rewards),\n",
    "                   np.min(episode_rewards),\n",
    "                   np.max(episode_rewards), dist_entropy,\n",
    "                   value_loss, action_loss))\n",
    "\n",
    "        training_stats['updates'].append(j)\n",
    "        training_stats['num_timesteps'].append(total_num_steps)\n",
    "        training_stats['mean_reward'].append(np.mean(episode_rewards))\n",
    "        training_stats['median_reward'].append(np.median(episode_rewards))\n",
    "        training_stats['min_reward'].append(np.min(episode_rewards))\n",
    "        training_stats['max_reward'].append(np.max(episode_rewards))\n",
    "\n",
    "    if (args.eval_interval is not None\n",
    "            and len(episode_rewards) > 1\n",
    "            and j % args.eval_interval == 0):\n",
    "        eval_envs = make_vec_envs(\n",
    "            args.env_name, args.seed + args.num_processes, args.num_processes,\n",
    "            args.gamma, eval_log_dir, args.add_timestep, device, True)\n",
    "\n",
    "        vec_norm = get_vec_normalize(eval_envs)\n",
    "        if vec_norm is not None:\n",
    "            vec_norm.eval()\n",
    "            vec_norm.ob_rms = get_vec_normalize(envs).ob_rms\n",
    "\n",
    "        eval_episode_rewards = []\n",
    "\n",
    "        obs = eval_envs.reset()\n",
    "        eval_recurrent_hidden_states = torch.zeros(args.num_processes,\n",
    "                        actor_critic.recurrent_hidden_state_size, device=device)\n",
    "        eval_masks = torch.zeros(args.num_processes, 1, device=device)\n",
    "\n",
    "        while len(eval_episode_rewards) < 10:\n",
    "            with torch.no_grad():\n",
    "                _, action, _, eval_recurrent_hidden_states = actor_critic.act(\n",
    "                    obs, eval_recurrent_hidden_states, eval_masks, deterministic=True)\n",
    "\n",
    "            # Obser reward and next obs\n",
    "            obs, reward, done, infos = eval_envs.step(action)\n",
    "\n",
    "            eval_masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                            for done_ in done]).to(device)\n",
    "            for info in infos:\n",
    "                if info is not None and 'episode' in info.keys():\n",
    "                    eval_episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        eval_envs.close()\n",
    "\n",
    "        print(\" Evaluation using {} episodes: mean reward {:.5f}\\n\".\n",
    "            format(len(eval_episode_rewards),\n",
    "                   np.mean(eval_episode_rewards)))\n",
    "\n",
    "    if args.vis and j % args.vis_interval == 0:\n",
    "        try:\n",
    "            # Sometimes monitor doesn't properly flush the outputs\n",
    "            win = visdom_plot(viz, win, args.log_dir, args.env_name,\n",
    "                              args.algo, args.num_env_steps)\n",
    "        except IOError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = make_rl_envs(args.env_name, args.num_processes, args.seed,  device)\n",
    "\n",
    "obs = envs.reset()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "episode_rewards = deque(maxlen=10)\n",
    "\n",
    "training_stats = {\n",
    "    'updates': [],\n",
    "    'num_timesteps': [],\n",
    "    'mean_reward': [],\n",
    "    'median_reward': [],\n",
    "    'min_reward': [],\n",
    "    'max_reward': []\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "for j in range(50):\n",
    "    for step in range(args.num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step],\n",
    "                    rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, infos = envs.step(action)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0]\n",
    "                                   for done_ in done])\n",
    "        rollouts.insert(obs, recurrent_hidden_states, action, action_log_prob, value, reward, masks)\n",
    "        \n",
    "#         obs = obs.values().clone()\n",
    "        obs = torch.FloatTensor(obs.cpu().data).cuda()\n",
    "        obs.requires_grad = True\n",
    "        next_value = actor_critic.get_value(obs,\n",
    "                                    rollouts.recurrent_hidden_states[-1],\n",
    "                                    rollouts.masks[-1])\n",
    "        next_value[0, ...].mean().backward()        \n",
    "        \n",
    "        im_display = np.concatenate(np.split(obs[0, ...].detach().cpu().numpy().transpose([1, 2, 0]).astype('uint8'), 4, axis=2), axis=1)\n",
    "        \n",
    "        print(\"reward\", reward[0])\n",
    "        print(\"action\", action[0])\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(im_display, cmap=\"gray\")        \n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(obs.grad[0,...].sum(dim=0).abs().cpu().numpy(), cmap=\"gray\")        \n",
    "        \n",
    "        plt.pause(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_display = np.concatenate(np.split(obs[0, ...].detach().cpu().numpy().transpose([1, 2, 0]).astype('uint8'), 4, axis=2), axis=1)\n",
    "plt.imshow(im_display)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
